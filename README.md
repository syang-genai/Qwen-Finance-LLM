2. model parallel training--data parallel(deepspeed), parallel checkpoint loading and saving. accelerate and deepspeed and model load/save. 
https://www.huaxiaozhuan.com/工具/huggingface_transformer/chapters/7_accelerate.html
https://zhuanlan.zhihu.com/p/711876344
https://discuss.huggingface.co/t/trainer-api-for-data-parallel-on-multi-node/138715
https://github.com/huggingface/blog/blob/main/zh/pytorch-ddp-accelerate-transformers.md
https://llamafactory.readthedocs.io/zh-cn/latest/advanced/distributed.html

 https://huggingface.co/docs/transformers/trainer
 https://huggingface.co/docs/transformers/accelerate
 https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed#multi-node-deepspeed
 
5. use large model+LoRA and distributed training framework, and bigger dataset.
6. evaluation performance improvement.
7. MOE
