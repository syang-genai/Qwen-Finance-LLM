  0%|                                                                                                                                                  | 0/4 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Traceback (most recent call last):
  File "/root/Qwen-Finance-LLM/main.py", line 120, in <module>
    main()
  File "/root/Qwen-Finance-LLM/main.py", line 116, in main
    trainer.train(resume_from_checkpoint = False)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2206, in train
    return inner_training_loop(
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 3749, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 3836, in compute_loss
    outputs = model(**inputs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
    loss = self.module(*inputs, **kwargs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
    return inner()
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
    result = forward_call(*args, **kwargs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 586, in forward
    logits = self.lm_head(hidden_states[:, slice_indices, :])
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.52 GiB. GPU 0 has a total capacity of 23.67 GiB of which 16.51 GiB is free. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.28 GiB is allocated by PyTorch, and 511.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/Qwen-Finance-LLM/main.py", line 120, in <module>
[rank0]:     main()
[rank0]:   File "/root/Qwen-Finance-LLM/main.py", line 116, in main
[rank0]:     trainer.train(resume_from_checkpoint = False)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2206, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 3749, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 3836, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/utils/generic.py", line 943, in wrapper
[rank0]:     output = func(self, *args, **kwargs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 586, in forward
[rank0]:     logits = self.lm_head(hidden_states[:, slice_indices, :])
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/Qwen-Finance-LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.52 GiB. GPU 0 has a total capacity of 23.67 GiB of which 16.51 GiB is free. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.28 GiB is allocated by PyTorch, and 511.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
