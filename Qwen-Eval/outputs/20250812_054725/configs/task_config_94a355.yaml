analysis_report: false
api_key: EMPTY
api_url: null
chat_template: null
dataset_args:
  data_collection: {}
  general_qa:
    dataset_id: ../dataset/eval_dataset
    description: A general question answering dataset for custom evaluation. For detailed
      instructions on how to use this benchmark, please refer to the [User Guide](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/llm.html#qa).
    eval_split: test
    extra_params: {}
    few_shot_num: 0
    few_shot_random: false
    filters: null
    metric_list:
    - AverageBLEU
    - AverageRouge
    model_adapter: generation
    name: general_qa
    output_types:
    - generation
    pretty_name: General-QA
    prompt_template: '请回答问题

      {query}'
    query_template: null
    subset_list:
    - BigScienceP3/BSP3_QA
    - Diweanshu/Finance-Reasoning/FR
    - H3Instruct/H3I
    - Josephgflowers/Finance-Instruct-500k-Formated/FIF
    - Open-R1/OpenMathReasoning/OMR
    - ZennyKenny/SyntheticFinancialDecisionsReasoningDataset/SFDR
    system_prompt: null
    tags:
    - QA
    - Custom
    train_split: null
dataset_dir: /root/.cache/modelscope/hub/datasets
dataset_hub: modelscope
datasets:
- data_collection
- general_qa
debug: false
dry_run: false
eval_backend: Native
eval_batch_size: 32
eval_config: null
eval_type: checkpoint
generation_config:
  chat_template_kwargs:
    enable_thinking: false
  max_tokens: 20000
  n: 1
ignore_errors: false
judge_model_args: {}
judge_strategy: auto
judge_worker_num: 1
limit: 128
mem_cache: false
model: qwen_model
model_args:
  precision: torch.float16
  revision: master
model_id: qwen_model
model_task: text_generation
outputs: null
seed: 42
stage: all
stream: false
template_type: null
timeout: null
use_cache: null
work_dir: ./outputs/20250812_054725
